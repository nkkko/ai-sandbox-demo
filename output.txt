### This is START of the README.md file:
~~~
# AI Demo Project Using SDE Sandbox

## Project Overview

Welcome to our AI Demo Project, showcasing the integration and application of AI technologies in Python within SDE sandbox. This project features capabilities for generating articles using OpenAI's API, and for fetching, storing, and searching web content using [Chroma DB](https://docs.trychroma.com/).

### Main Features:
- **Sitemap Fetching and Parsing**: Automates the process of fetching sitemap XML from websites and parsing it to extract URLs.
- **Content Extraction and Storage**: Retrieves and stores web page content in ChromaDB, a versatile database system.
- **AI-Powered Search**: Employs OpenAI's embedding functions for efficient and intelligent content search within ChromaDB.
- **Article Generation**: Generates articles based on user prompts and search terms, utilizing OpenAI's GPT-4 turbo model.

## Getting Started

### Prerequisites
- Access to an SDE such as Daytona.io which supports Dev Container Specification
- Python >3.10
- OpenAI API key

### Installation
There are two ways to set up the environment for this project:

1. **Using an SDE**:
   - Users of Daytona.io, Codeanywhere, Codespaces, or VS Code can auto-configure their environment with devcontainer.json, allowing for instant start.
   - Just point the SDE to the Git repository url: https://github.com/nkkko/ai-sandbox-demo
   - Set up an `.env` file with your `OPENAI_API_KEY`.

2. **Manual Setup**:
   - Clone the repository to your workspace or local environment.
   - Install necessary packages:
     ```
     pip install -r requirements.txt
     ```
     Or run:
     ```
     pip install openai chromadb python-dotenv bs4 argparse lxml
     ```
   - Set up an `.env` file with your `OPENAI_API_KEY` or don't and just use the default embeddings function (all-MiniLM-L6-v2).

### Usage

1. **populate.py**: 
   - Run the script with a sitemap URL to fetch, parse, and store website content. Optionally select the embeddings function to use with `--ef`.
   - Usage: `python populate.py [SITEMAP_URL] [--n [MAX_URLS]] [--ef {default|openai}]`
   - Example: `python populate.py https://www.example.com/sitemap.xml --n 100 --ef openai`

2. **search.py**:
   - Perform searches in the stored data. Optionally select the embeddings function to use with `--ef`.
   - Usage: `python search.py [SEARCH_QUERY] [--n [NUMBER_OF_RESULTS]] [--ef {default|openai}]`
   - Example: `python search.py "example search query" --n 5 --ef default`

3. **write.py**:
   - Generate articles based on a prompt and ChromaDB search terms. Optionally select the embeddings function to use with `--ef`.
   - Usage: `python write.py [PROMPT] --s [SEARCH_TERM] --n [NUMBER_OF_RESULTS] [--ef {default|openai}]`
   - Example: `python write.py "Write an article about AI" --s "artificial intelligence" --n 3 --ef openai`

The `--ef` argument allows you to specify which [embeddings function](https://docs.trychroma.com/embeddings) to use when interacting with ChromaDB. By default, `default_ef` is used (all-MiniLM-L6-v2). If you want to use OpenAI's embeddings, specify `--ef openai`.

## Structure

- `SynthSpyder.py`: Core module containing the logic for fetching, parsing, storing, and searching.
- `populate.py`: Script for populating ChromaDB with content from a sitemap.
- `search.py`: Script to search within the stored data.
- `write.py`: Script to generate articles using OpenAI and ChromaDB to fetch context.
- `db/`: Directory containing ChromaDB client and utilities.
- `.env`: Environment file for storing your OpenAI API key.
- `.devcontainer`: Configuration directory with file for setting up the development environment automatically in supported SDEs.

## Examples

- Fetch and store 10 first articles from the sitemap:
  ```bash
    python populate.py https://www.daytona.io/sitemap-definitions.xml --n 10
  ```

- Search for the first two terms that are closest to the query in the stored content:
  ```bash
    python search.py "SDE" --n 2
  ```

- Generate an article within set context of the first result from the vector DB: 
  ```bash
    python write.py "Tell me a joke about" --s "guardrails" --n 1
  ```
  ```bash
    Why was the developer afraid to play cards with the guardrails?

    Because every time they tried to deal, the guardrails kept reminding them to stay within their limits! ðŸš§ðŸ˜„
  ```

## Contributing
Contributions to enhance this demo project are welcomed. Please adhere to standard coding practices and provide documentation for your contributions.

## License
[MIT License](LICENSE.md)

## Acknowledgments
- Thanks to OpenAI for fantastic embeddings.
- Appreciation to the developers of ChromaDB.

---

*This README is part of an AI Demo Project using SDE as a sandbox environment for AI projects. It aims to guide users through the setup, usage, and contribution to the project.*
~~~
### This is END of the file README.md!

### This is START of the write.py file:
~~~
from SynthSpyder import search_in_chromadb, write_article, default_ef, openai_ef, chroma_client, collection_name
import argparse

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Generate an article using OpenAI and ChromaDB.')
    parser.add_argument('prompt', type=str, help='Article prompt.')
    parser.add_argument('--s', type=str, required=True, help='Search term for ChromaDB.')
    parser.add_argument('--n', type=int, default=5, help='Number of ChromaDB search results.')
    parser.add_argument('--ef', type=str, default='default', choices=['default', 'openai'], help='Embedding function to use (default or openai).')
    
    args = parser.parse_args()

    # Select the embedding function based on the user input
    if args.ef == "openai":
        embedding_function = openai_ef
    else:
        embedding_function = default_ef

    # Initialize the collection with the selected embedding function
    collection = chroma_client.get_or_create_collection(
        name=collection_name,
        embedding_function=embedding_function
    )

    # Search in ChromaDB
    search_results = search_in_chromadb(args.s, args.n, collection)

    # Process and append search results to the prompt
    additional_context = ""
    for i in range(len(search_results['documents'])):  # Iterate over each set of documents (outer list)
        for j in range(len(search_results['documents'][i])):  # Iterate over documents in each set (inner list)
            additional_context += f"{search_results['documents'][i][j]}\n\n"

    combined_prompt = args.prompt + "\n\n" + additional_context

    # Generate the article
    article = write_article(combined_prompt)
    print(article)

~~~
### This is END of the file write.py!

### This is START of the populate.py file:
~~~
from SynthSpyder import fetch_sitemap, parse_sitemap, fetch_and_save_html, default_ef, openai_ef, chroma_client, collection_name
import argparse
import asyncio
import logging
from tqdm import tqdm

async def main(sitemap_url, n, embedding_function_name):
    global collection
    """
    Main function to fetch, parse the sitemap, and save HTML content to ChromaDB.
    """
    # Select the embedding function based on the user input
    if embedding_function_name == "openai":
        embedding_function = openai_ef
    else:
        embedding_function = default_ef

    # Get or create the collection with the selected embedding function
    collection = chroma_client.get_or_create_collection(
        name=collection_name,
        embedding_function=embedding_function
    )

    # Fetch the sitemap
    sitemap_xml = await fetch_sitemap(sitemap_url)
    if not sitemap_xml:
        logging.error("Failed to fetch sitemap.")
        return
    else:
        print(f"Successfully fetched: {sitemap_url}")

    # Parse the sitemap to get URLs
    urls = parse_sitemap(sitemap_xml, n)
    if not urls:
        logging.error("No URLs found in sitemap.")
        return

    # Set up the progress bar
    pbar = tqdm(total=len(urls))

    # Function to update the progress bar
    def update_progress():
        pbar.update(1)

    # Fetch and save HTML content of each URL
    tasks = [fetch_and_save_html(url, update_progress, collection) for url in urls]
    await asyncio.gather(*tasks)
    
    pbar.close()

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Fetch Sitemap Content.')
    parser.add_argument('sitemap_url', type=str, help='Full URL of sitemap.xml file.')
    parser.add_argument('--n', type=int, default=None, help='Number of results to return.')
    parser.add_argument('--ef', type=str, default='default', choices=['default', 'openai'], help='Embedding function to use (default or openai).')
    args = parser.parse_args()

    asyncio.run(main(args.sitemap_url, args.n, args.ef))
~~~
### This is END of the file populate.py!

### This is START of the search.py file:
~~~
from SynthSpyder import search_in_chromadb, default_ef, openai_ef, chroma_client, collection_name
import argparse

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Search in ChromaDB.')
    parser.add_argument('query', type=str, help='Search query.')
    parser.add_argument('--n', type=int, default=5, help='Number of results to return.')
    parser.add_argument('--ef', type=str, default='default', choices=['default', 'openai'], help='Embedding function to use (default or openai).')
    args = parser.parse_args()

    # Select the embedding function based on the user input
    if args.ef == "openai":
        embedding_function = openai_ef
    else:
        embedding_function = default_ef

    # Initialize the collection with the selected embedding function
    collection = chroma_client.get_or_create_collection(
        name=collection_name,
        embedding_function=embedding_function
    )

    # Perform the search
    results = search_in_chromadb(args.query, args.n, collection)

# Print the results
for i in range(len(results['documents'])):  # Iterate over each set of documents (outer list)
    for j in range(len(results['documents'][i])):  # Iterate over documents in each set (inner list)
        print(f"URL: {results['metadatas'][i][j]['url']}")
        print(f"Content: {results['documents'][i][j]}")  # Print the document
        print("-" * 50)
~~~
### This is END of the file search.py!

### This is START of the SynthSpyder.py file:
~~~
# pip install openai chromadb python-dotenv bs4 argparse lxml
import logging
import argparse
import asyncio
from bs4 import BeautifulSoup
import chromadb
from chromadb.db.base import UniqueConstraintError  # Import the exception
from chromadb.utils import embedding_functions
from dotenv import load_dotenv
import json
import openai
from openai import OpenAI
import os
import requests
import xml.etree.ElementTree as ET
from tqdm import tqdm

# Get the root logger
logger = logging.getLogger()
logger.setLevel(logging.INFO)  # Set the logging level to INFO

# Load environment variables
load_dotenv()

# Set your OpenAI key and configure OpenAI Client
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
openai_client = OpenAI(api_key=OPENAI_API_KEY)

# Embeddings functions
default_ef = embedding_functions.DefaultEmbeddingFunction()
openai_ef = embedding_functions.OpenAIEmbeddingFunction(
                api_key=OPENAI_API_KEY,
                model_name="text-embedding-ada-002"
            )

# Initialize ChromaDB Client
# chroma_client = chromadb.Client() # in-memory db
chroma_client = chromadb.PersistentClient(path="db") # persistent db
collection_name = "sitemap_collection"

async def fetch_sitemap(url):
    """
    Asynchronously fetch a sitemap from the given URL.
    Returns the sitemap's XML content as a string, or None if an error occurs.
    """
    try:
        response = await asyncio.to_thread(requests.get, url)
        response.raise_for_status()
        return response.text
    except requests.RequestException as e:
        logging.error(f"Error fetching sitemap: {e}")
        return None

def parse_sitemap(sitemap_content, max_urls=None):
    """
    Parse the sitemap content and extract a limited number of URLs.

    Args:
    sitemap_content (str): XML content of the sitemap.
    max_urls (int, optional): Maximum number of URLs to extract. If None, extracts all URLs.

    Returns:
    List[str]: A list of extracted URLs, limited to 'max_urls' if specified.
    """
    import xml.etree.ElementTree as ET

    # Parse the XML content
    tree = ET.ElementTree(ET.fromstring(sitemap_content))
    root = tree.getroot()

    # Define the namespace map for parsing sitemap XML
    # Adjusted to handle both http and https namespaces
    namespaces = {
        'http': 'http://www.sitemaps.org/schemas/sitemap/0.9',
        'https': 'https://www.sitemaps.org/schemas/sitemap/0.9'
    }

    # Try to extract the URLs with http namespace first
    urls = [element.text for element in root.findall('.//http:loc', namespaces)]
    
    # If no URLs found, try with https namespace
    if not urls:
        urls = [element.text for element in root.findall('.//https:loc', namespaces)]

    # Limit the number of URLs if max_urls is specified
    if max_urls is not None:
        urls = urls[:max_urls]

    return urls

async def fetch_and_save_html(url, update_progress, collection):
    """
    Fetch the HTML content of a given URL, extract and clean text from main content elements, 
    and save it to ChromaDB.
    """
    try:
        response = await asyncio.to_thread(requests.get, url)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'lxml')
        
        # Remove script and style elements
        for script_or_style in soup(['script', 'style', 'header', 'footer', 'nav']):
            script_or_style.extract()
        
        # Extract main content using common content markers
        main_content = soup.find_all(['article', 'main', 'div'], class_=lambda x: x and 'content' in x)
        
        # If no common content markers found, fall back to extracting all text
        if not main_content:
            main_content = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p'])
        
        text_content = ' '.join(element.get_text(strip=True, separator=' ') for element in main_content)

        save_to_chromadb(url, text_content, collection)
        update_progress()
    except requests.RequestException as e:
        logging.error(f"Error fetching HTML content from {url}: {e}")


def save_to_chromadb(url, html_content, collection):
    """
    Save the HTML content to ChromaDB with detailed error handling.
    """
    try:
        collection.upsert(
            documents=[html_content],
            metadatas=[{"url": url}],
            ids=[url]
        )
    except UniqueConstraintError as e:
        logging.warning(f"Duplicate entry for {url} not added to ChromaDB: {e}")
    except Exception as e:
        # Log the exception type and message
        logging.error(f"Exception while adding/updating {url} in ChromaDB: {type(e).__name__}, {e}")

        # Optionally, log a snippet of the content for further inspection
        snippet = html_content[:200]  # Adjust the length as needed
        logging.info(f"Content snippet: {snippet}")

def search_in_chromadb(query, n_results, collection):
    """
    Search in ChromaDB for the given query.

    Args:
    query (str): The search query.
    n_results (int): Number of search results to return.

    Returns:
    List of search results.
    """

    # Search in the collection
    search_results = collection.query(
        query_texts=[query],
        n_results=n_results
    )

    return search_results

def write_article(prompt):
    """
    Generate an article using the OpenAI API.
    """
    try:
        response = openai_client.chat.completions.create(
            model='gpt-4-1106-preview',
            messages=[
                {"role": "system", "content": "Follow user instructions. Write using Markdown."},
                {"role": "user", "content": prompt}
            ]
        )
        # Access the 'content' attribute of the last message in the response
        last_message_content = response.choices[0].message.content
        return last_message_content

    except Exception as e:
        print(f"An error occurred during article generation: {e}")
        return None
~~~
### This is END of the file SynthSpyder.py!

